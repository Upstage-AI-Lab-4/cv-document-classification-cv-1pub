{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import easyocr\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam,NAdam\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.amp import GradScaler,autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import augraphy\n",
    "from augraphy import AugraphyPipeline, NoiseTexturize,DirtyDrum,InkBleed,LightingGradient,SubtleNoise,BleedThrough, BadPhotoCopy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from timm import create_model\n",
    "\n",
    "from datetime import datetime\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameter, Const Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 2024\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "# model config\n",
    "model_name = 'efficientnetv2_rw_m' # 'resnet50' 'efficientnet-b0', ...\n",
    "# training config\n",
    "img_size = 480\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "num_workers = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Class Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, pipeline = None, transform=None, is_train = True):\n",
    "       # CSV 파일에서 데이터 로드\n",
    "        self.df = pd.read_csv(csv)\n",
    "        self.df = self._fix_train_dataframe(self.df).values\n",
    "        self.path = path\n",
    "        self.pipeline = pipeline\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.targets = self._fix_train_dataframe(pd.read_csv(csv))['target'].values\n",
    "        \n",
    "    def _fix_train_dataframe(self, df):\n",
    "        train_df = df\n",
    "        train_df.loc[train_df['ID'] == '45f0d2dfc7e47c03.jpg', 'target'] = 7  #from 3\n",
    "        train_df.loc[train_df['ID'] == 'aec62dced7af97cd.jpg', 'target'] = 14 #from 3\n",
    "        train_df.loc[train_df['ID'] == '8646f2c3280a4f49.jpg', 'target'] = 3  #from 7\n",
    "        train_df.loc[train_df['ID'] == '1ec14a14bbe633db.jpg', 'target'] = 7  #from 14\n",
    "        return train_df\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.df)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 파일 경로 구성\n",
    " \n",
    "        img_name, label = self.df[idx]\n",
    "        img_path = os.path.join(self.path, img_name)\n",
    "        image = cv2.imread(img_path, cv2.COLOR_BGR2RGB)\n",
    "        #augraphy\n",
    "        if self.pipeline:\n",
    "            image = self.pipeline(image)\n",
    "        # 변환 적용\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']  # 'image=image'로 albumentations 호출\n",
    " \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation을 위한 transform 코드\n",
    "trn_transform = A.Compose([\n",
    "    #A.Resize(height=img_size, width=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=(255,255,255)),\n",
    "    A.Transpose(always_apply=False, p=0.5),\n",
    "    A.Rotate(limit=90, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n",
    "    A.RGBShift(p=0.5),\n",
    "    A.ChannelShuffle(p=0.5),\n",
    "    A.GlassBlur( sigma = 0.7 , max_delta = 4 , iterations = 2 , always_apply = False , mode = 'fast' , p = 0.5 ),\n",
    "    A.Cutout(p=0.5, num_holes=8, max_h_size=48, max_w_size=48),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=(255,255,255)),\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# test image 변환을 위한 transform 코드\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 정의\n",
    "trn_dataset = ImageDataset(\n",
    "    \"../data/train_augmented.csv\",\n",
    "    \"../data/train_augmented/\",\n",
    "    transform=trn_transform,\n",
    "    #pipeline=augraphy_pipeline,\n",
    "    is_train = True\n",
    ")\n",
    "\n",
    "tst_dataset = ImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    transform=tst_transform,\n",
    "    is_train = False\n",
    ")\n",
    "print(len(trn_dataset), len(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 정의\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Function Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, save_path, filename):\n",
    "    \"\"\"\n",
    "    모델을 pkl 파일로 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: 저장할 모델\n",
    "        save_path: 저장할 디렉토리 경로\n",
    "        filename: 저장할 파일 이름\n",
    "    \"\"\"\n",
    "    # 저장 경로가 없으면 생성\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # 전체 경로 생성\n",
    "    full_path = os.path.join(save_path, filename)\n",
    "    \n",
    "    # 모델의 state_dict와 추가 정보를 저장\n",
    "    save_dict = {\n",
    "        'model_name': model.default_cfg['architecture'],  # timm 모델의 아키텍처 이름\n",
    "        'state_dict': model.state_dict(),\n",
    "        'num_classes': model.num_classes,\n",
    "    }\n",
    "    \n",
    "    # pkl 파일로 저장\n",
    "    with open(full_path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    \n",
    "    print(f\"Model saved successfully to {full_path}\")\n",
    "\n",
    "def load_model(load_path, filename, device='cuda'):\n",
    "    \"\"\"\n",
    "    저장된 pkl 파일에서 모델을 불러옵니다.\n",
    "    \n",
    "    Args:\n",
    "        load_path: 불러올 파일이 있는 디렉토리 경로\n",
    "        filename: 불러올 파일 이름\n",
    "        device: 모델을 로드할 디바이스\n",
    "    \n",
    "    Returns:\n",
    "        loaded_model: 불러온 모델\n",
    "    \"\"\"\n",
    "    # 전체 경로\n",
    "    full_path = os.path.join(load_path, filename)\n",
    "    \n",
    "    # pkl 파일 로드\n",
    "    with open(full_path, 'rb') as f:\n",
    "        save_dict = pickle.load(f)\n",
    "    \n",
    "    # 동일한 구조의 모델 생성\n",
    "    model = timm.create_model(\n",
    "        save_dict['model_name'],\n",
    "        pretrained=False,\n",
    "        num_classes=save_dict['num_classes']\n",
    "    )\n",
    "    \n",
    "    # 저장된 가중치 로드\n",
    "    model.load_state_dict(save_dict['state_dict'])\n",
    "    \n",
    "    # 지정된 디바이스로 모델 이동\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded successfully from {full_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup 적용 함수\n",
    "def mixup_data(x, y, alpha=0.8):\n",
    "    \"\"\"Mixup 데이터 생성 함수\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# CutMix 적용 함수\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"CutMix 데이터 생성 함수\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    # 이미지의 사각형 영역을 자르고 섞기\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    \"\"\"CutMix bounding box 생성\"\"\"\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)  # np.int 대신 int로 변경\n",
    "    cut_h = int(H * cut_rat)  # np.int 대신 int로 변경\n",
    "\n",
    "    # 무작위 위치에서 시작하는 좌표\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# 손실 함수에서 Mixup/CutMix용 가중치를 적용\n",
    "def mixup_cutmix_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup/CutMix 손실 함수\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, scaler=None, accumulation_steps=1, augmentation_func=None):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader, total=len(loader))\n",
    "    for step, (image, targets) in enumerate(pbar):\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        with autocast(device_type='cuda',enabled=scaler is not None):  # Mixed precision training if scaler is provided\n",
    "\n",
    "            # Mixup 또는 CutMix 적용\n",
    "            if augmentation_func:\n",
    "                images, targets_a, targets_b, lam = augmentation_func(image, targets)\n",
    "                preds = model(images) / accumulation_steps\n",
    "                loss = mixup_cutmix_criterion(loss_fn, preds, targets_a, targets_b, lam)\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "            else:\n",
    "                # Augmentation이 없을 경우 일반 학습\n",
    "                preds = model(image)\n",
    "                loss = loss_fn(preds, targets) / accumulation_steps\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward() if scaler else loss.backward()\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        # Progress bar with running loss\n",
    "        avg_loss = train_loss / (step + 1)\n",
    "        pbar.set_description(f\"Batch Loss: {loss.item() * accumulation_steps:.4f}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Final metrics\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수 정의\n",
    "def validate_one_epoch(loader, model, loss_fn, device):\n",
    "    model.eval()  # 평가 모드로 전환\n",
    "    val_loss = 0\n",
    "    preds_list = []    \n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():  # 검증 시에는 gradient 계산 필요 없음\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # 예측\n",
    "            preds = model(images)\n",
    "            logits = preds.logits if hasattr(preds, 'logits') else preds  # logits 추출\n",
    "            loss = loss_fn(logits, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # 예측값과 실제값을 리스트에 저장\n",
    "            preds_list.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            targets_list.extend(targets.cpu().numpy())\n",
    "\n",
    "    # 평균 손실과 성능 지표 계산\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score - self.min_delta:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 저장을 위한 리스트 초기화\n",
    "all_predictions = []\n",
    "\n",
    "normal_score_list = []\n",
    "best_score = 0\n",
    "best_normal_score = 0\n",
    "save_path = '../output/models'\n",
    "\n",
    "# K-fold 교차 검증 루프\n",
    "k_folds = 3\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=2024)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(trn_dataset, trn_dataset.targets)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    best_score = 0\n",
    "    best_normal_score = 0\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_name = f\"fold_{fold+1}_{current_time}\"  # 예: fold_1_20231105_153045 형식\n",
    "    \n",
    "    # Train/Validation Subset 생성\n",
    "    train_subset = Subset(trn_dataset, train_idx)\n",
    "    val_subset = Subset(trn_dataset, val_idx)\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # 모델, 옵티마이저, 손실 함수 초기화\n",
    "    model = timm.create_model(model_name,pretrained=True,num_classes=17).to(device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=LR, weight_decay=1e-5, amsgrad=True)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-4)\n",
    "    early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "\n",
    "    wandb.init(project=f\"{model_name}_batch{BATCH_SIZE}_worker{num_workers}\", \n",
    "               name=run_name,  # 폴드 번호와 날짜 및 시간 포함\n",
    "               group=f\"{model_name}_kfold\",\n",
    "               config={\n",
    "                   \"fold\": fold + 1,\n",
    "                   \"model_name\": model_name,\n",
    "                   \"img_size\": img_size,\n",
    "                   \"epochs\": EPOCHS,\n",
    "                   \"batch_size\": BATCH_SIZE,\n",
    "                   \"learning_rate\": LR,\n",
    "                   \"optimizer\": \"Adam\",\n",
    "                   \"scheduler\": \"CosineAnnealingLR\",\n",
    "                   \"loss_function\": \"CrossEntropyLoss\",\n",
    "               })\n",
    "\n",
    "    # 학습 및 검증 루프\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}: Applying Mixup\")\n",
    "        augmentation_func = mixup_data\n",
    "            \n",
    "        # Training\n",
    "        ret = train_one_epoch(train_loader, model, optimizer, loss_fn, device=device, augmentation_func=augmentation_func)\n",
    "        ret['epoch'] = epoch\n",
    "\n",
    "        val_metrics = validate_one_epoch(val_loader, model, loss_fn, device=device)\n",
    "        val_metrics['epoch'] = epoch\n",
    "\n",
    "        # 스케줄러 스텝 (CosineAnnealingLR 사용 시)\n",
    "        scheduler.step()  # 또는 ReduceLROnPlateau를 사용하는 경우: scheduler.step(val_metrics['val_loss'])\n",
    "\n",
    "        # 로그 출력\n",
    "        log = f\"Fold {fold + 1}, Epoch {epoch+1}/{EPOCHS}\\n\"\n",
    "        for k, v in ret.items():\n",
    "            log += f\"Train {k}: {v:.4f}\\n\"\n",
    "        for k, v in val_metrics.items():\n",
    "            log += f\"Val {k}: {v:.4f}\\n\"\n",
    "        print(log)\n",
    "\n",
    "        save_model(model, save_path, f'model_name.pkl')\n",
    "    \n",
    "        preds_list = []\n",
    "        model.eval()\n",
    "        for image, _ in tqdm(tst_loader):\n",
    "            image = image.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "        pred_df['target'] = preds_list\n",
    "\n",
    "        pred_df.to_csv(f'../output/output/pred_name.csv', index=False)\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_loss\": ret['train_loss'],\n",
    "            \"train_acc\": ret['train_acc'],\n",
    "            \"train_f1\": ret['train_f1'],\n",
    "            \"valid_loss\": val_metrics['val_loss'],\n",
    "            \"valid_acc\": val_metrics['val_acc'],\n",
    "            \"valid_f1\": val_metrics['val_f1'],\n",
    "            \"epoch\": epoch,\n",
    "            \"best_custom_score\": best_normal_score\n",
    "        })\n",
    "        # Early Stopping 체크\n",
    "        if early_stopping(val_metrics[\"val_loss\"]):\n",
    "            # 각 폴드의 최적 예측 결과를 수집\n",
    "            print(f\"Early stopping triggered for fold {fold + 1}\")\n",
    "            break\n",
    "    \n",
    "    # 각 폴드의 학습 완료 후 전체 테스트 데이터셋(tst_loader)에서 예측 수행\n",
    "    fold_predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tst_loader:  # targets는 사용하지 않으므로 _로 처리\n",
    "            images = images.to(device)\n",
    "            preds = model(images)  # 직접 예측 값 사용\n",
    "            fold_predictions.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "    \n",
    "    # 테스트 데이터에 대한 폴드 예측 결과 저장\n",
    "    all_predictions.append(fold_predictions)    \n",
    "    wandb.finish()\n",
    "    print(f\"Fold {fold + 1} completed\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 최종 예측값을 저장할 리스트 초기화\n",
    "final_predictions = []\n",
    "\n",
    " #각 인덱스별로 최빈값을 찾아서 저장 #하드보팅\n",
    "for idx in range(len(all_predictions[0])):  # 각 테스트 데이터의 인덱스\n",
    "    # 해당 인덱스에 대한 각 폴드의 예측값 리스트 생성\n",
    "    preds_for_idx = [all_predictions[fold][idx] for fold in range(len(all_predictions))]\n",
    "    \n",
    "    # 최빈값 계산\n",
    "    counts = Counter(preds_for_idx)\n",
    "    most_common = counts.most_common()\n",
    "    max_count = most_common[0][1]\n",
    "    \n",
    "    # 최빈값 중 가장 마지막 값 선택\n",
    "    candidates = [val for val, count in most_common if count == max_count]\n",
    "    final_value = candidates[-1]  # 가장 마지막에 등장한 값을 선택\n",
    "    \n",
    "    # 최종 예측에 저장\n",
    "    final_predictions.append(final_value)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"Final predictions length: {len(final_predictions)}\")  # 3140인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 예측 결과를 DataFrame으로 저장\n",
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])  # test 데이터셋의 ID와 target 컬럼 사용\n",
    "pred_df['target'] = final_predictions  # 예측 값을 'target' 열에 삽입\n",
    "\n",
    "# sample_submission.csv와 ID 일치 여부 확인 (선택사항)\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all(), \"ID가 일치하지 않습니다!\"\n",
    "\n",
    "# CSV로 저장\n",
    "output_path = \"../output/output\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "pred_df.to_csv(os.path.join(output_path, \"pred_result.csv\"), index=False)\n",
    "print(\"예측 결과가 '../output/output/pred_result.csv'에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
