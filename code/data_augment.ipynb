{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augraphy 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from augraphy import AugraphyPipeline, NoiseTexturize, DirtyDrum, InkBleed, LightingGradient, SubtleNoise, BleedThrough,BadPhotoCopy\n",
    "from concurrent.futures import ThreadPoolExecutor,as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "origin_df = pd.read_csv('../data/train.csv')\n",
    "# 1. Augraphy 파이프라인 설정\n",
    "augraphy_pipeline = AugraphyPipeline([\n",
    "    NoiseTexturize(\n",
    "        sigma_range=(10, 15),             # 노이즈의 강도를 높게 설정 (뚜렷한 텍스처)\n",
    "        turbulence_range=(2, 4),          # 약간의 불규칙성 (자연스러운 패턴)\n",
    "        texture_width_range=(150, 200),   # 중간 정도의 텍스처 크기\n",
    "        texture_height_range=(150, 200),\n",
    "        p=1.0                        # 효과 적용 확률 (100% 적용)\n",
    "    )\n",
    "])\n",
    "\n",
    "# 2. 이미지 경로 및 저장 경로 설정\n",
    "input_dir = '../data/train'\n",
    "output_dir = \"../data/train_augmented\" # 저장할 디렉토리\n",
    "\n",
    "# 저장 디렉토리 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def process_and_save_image(filename):\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    image = cv2.imread(img_path)\n",
    "    augmented_image = augraphy_pipeline(image)\n",
    "    output_path = os.path.join(output_dir, f\"aug_texture_{filename}\")\n",
    "    cv2.imwrite(output_path, augmented_image)\n",
    "\n",
    "    return output_path\n",
    "# 멀티스레딩을 통해 이미지 처리 (진행률 표시)\n",
    "filenames = [f for f in os.listdir(input_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "# tqdm을 사용하여 진행률 표시\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    # 각 작업을 제출하고 Future 객체를 저장\n",
    "    futures = {executor.submit(process_and_save_image, filename): filename for filename in filenames}\n",
    "    # 진행률 표시\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc='Processing Images'):\n",
    "        filename = futures[future]\n",
    "        try:\n",
    "            result = future.result()  # 결과를 가져오고\n",
    "            #print(f\"Saved: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albumentation 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_blur = A.MedianBlur(blur_limit=17, p=1.0)\n",
    "motion_blur = A.MotionBlur(blur_limit=17, p=1.0)\n",
    "random_brightness_contrast = A.RandomBrightnessContrast(brightness_limit=(-0.8, 0.8), contrast_limit=0.2, p=1.0)\n",
    "hue_saturation = A.HueSaturationValue(hue_shift_limit=60, sat_shift_limit=60, val_shift_limit=60, p=1.0)\n",
    "defocus = A.Defocus(p=1.0)\n",
    "gamma =A.RandomGamma(p=1.0)\n",
    "max_gauss_noise = A.GaussNoise(var_limit=(800,1500), p=1.0)\n",
    "rot90 = A.Rotate(limit=(90,90), p=1.0)\n",
    "rot180 = A.Rotate(limit=(180,180), p=1.0)\n",
    "rot270 = A.Rotate(limit=(270,270), p=1.0)\n",
    "#\n",
    "horizontal_flip = A.HorizontalFlip(p=1.0)  # 항상 Horizontal Flip 적용\n",
    "vertical_flip = A.VerticalFlip(p=1.0)      # 항상 Vertical Flip 적용\n",
    "#\n",
    "input_dir = \"../data/train\"  # 원본 이미지가 있는 폴더\n",
    "output_dir = \"../data/train_augmented/\"  # 증강된 이미지를 저장할 폴더\n",
    "os.makedirs(output_dir, exist_ok=True)  # 저장 폴더가 없으면 생성\n",
    "\n",
    "#위에서 한번 증강을 했고, 해당 증강된 데이터에 대해 처리해야하기 때문에 output에서 파일 읽기\n",
    "files_with_keyword = [f for f in os.listdir(output_dir)]\n",
    "for img_name in tqdm(files_with_keyword):\n",
    "    img_path = os.path.join(output_dir, img_name)\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Could not read image {img_name}. Skipping.\")\n",
    "        continue\n",
    "    image_flip = horizontal_flip(image=image)[\"image\"]\n",
    "\n",
    "    rot01 = rot90(image=image)[\"image\"]\n",
    "    rot01_filename = f\"rot90_nt_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, rot01_filename), rot01)\n",
    "    \n",
    "    rot02 = rot180(image=image)[\"image\"]\n",
    "    rot02_filename = f\"rot180_nt_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, rot02_filename), rot02)\n",
    "\n",
    "    rot03 = rot270(image=image)[\"image\"]\n",
    "    rot03_filename = f\"rot270_nt_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, rot03_filename), rot03)\n",
    "\n",
    "    frot01 = rot90(image=image_flip)[\"image\"]\n",
    "    frot01_filename = f\"frot90_nt_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, frot01_filename), frot01)\n",
    "    \n",
    "    frot02 = rot180(image=image_flip)[\"image\"]\n",
    "    frot02_filename = f\"frot180_nt_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, frot02_filename), frot02)\n",
    "\n",
    "    frot03 = rot270(image=image_flip)[\"image\"]\n",
    "    frot03_filename = f\"frot270_nt_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, frot03_filename),frot03)\n",
    "\n",
    "    # MedianBlur 적용\n",
    "    med_blur = median_blur(image=image)[\"image\"]\n",
    "    med_blur_filename = f\"med_blur_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, med_blur_filename), med_blur)\n",
    "    \n",
    "    # Motion Blur 적용\n",
    "    mot_blur = motion_blur(image=image)[\"image\"]\n",
    "    mot_blur_filename = f\"mot_blur_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, mot_blur_filename), mot_blur)\n",
    "    \n",
    "    # Random Brightness Contrast 적용\n",
    "    bright_contrast = random_brightness_contrast(image=image)[\"image\"]\n",
    "    bright_contrast_filename = f\"bright_contrast_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, bright_contrast_filename), bright_contrast)\n",
    "    \n",
    "    # Hue Saturation Value 적용\n",
    "    hue_sat = hue_saturation(image=image)[\"image\"]\n",
    "    hue_sat_filename = f\"hue_sat_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, hue_sat_filename), hue_sat)\n",
    "    \n",
    "    # Defocus 적용\n",
    "    defoc = defocus(image=image)[\"image\"]\n",
    "    defoc_filename = f\"defocus_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, defoc_filename), defoc)\n",
    "    \n",
    "    # Gamma 적용\n",
    "    gm = gamma(image=image)[\"image\"]\n",
    "    gamma_filename = f\"gamma_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, gamma_filename), gm)\n",
    "\n",
    "    # Very Strong Gaussian Noise 적용\n",
    "    m_gn = max_gauss_noise(image=image)[\"image\"]\n",
    "    m_gauss_noise_filename = f\"very_strong_gauss_noise_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, m_gauss_noise_filename), m_gn)\n",
    "\n",
    "\n",
    "list_dir = os.listdir(output_dir)\n",
    "\n",
    "for img_name in tqdm(list_dir):\n",
    "    img_path = os.path.join(output_dir, img_name)\n",
    "    image = cv2.imread(img_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Could not read image {img_name}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Horizontal Flip 적용\n",
    "    h_flip = horizontal_flip(image=image)[\"image\"]\n",
    "    h_flip_filename = f\"hf_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, h_flip_filename), h_flip)\n",
    "\n",
    "    # Vertical Flip 적용\n",
    "    v_flip = vertical_flip(image=image)[\"image\"]\n",
    "    v_flip_filename = f\"vf_{img_name}\"\n",
    "    cv2.imwrite(os.path.join(output_dir, v_flip_filename), v_flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타 작업코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "df.apply(lambda row: shutil.copy(os.path.join('../data/train', row['ID']), '../data/train_augmented'), axis=1)\n",
    "#데이터 train에 있는 원본파일을 train_augmented에 복제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([f for f in os.listdir(output_dir) if f.endswith('.jpg')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 증강파일 labeling한 train_augmented dataframe 생성 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/train_augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 ID에 대해 prefix가 붙은 파일을 찾아 추가할 행들을 저장할 리스트 생성\n",
    "additional_rows = []\n",
    "\n",
    "# train_df의 각 row에 대해 처리\n",
    "for idx, row in train_df.iterrows():\n",
    "    base_id = row['ID']\n",
    "    target = row['target']\n",
    "    \n",
    "    # train 디렉토리의 파일명들을 순회하며, prefix가 붙은 파일명을 찾음\n",
    "    for filename in os.listdir(train_dir):\n",
    "        # prefix가 붙은 파일 중 원본 ID와 일치하는 파일만 선택\n",
    "        if filename.endswith(base_id) and filename != base_id:\n",
    "            # 추가할 행을 리스트에 저장\n",
    "            additional_rows.append({'ID': filename, 'target': target})\n",
    "\n",
    "# 기존 train_df에 추가된 행들을 추가\n",
    "new_train_df = pd.concat([train_df, pd.DataFrame(additional_rows)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df.to_csv('../data/train_augmented.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
